{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1592754-c5ff-410b-a1ea-81a46d837fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer-slim->transformers) (8.3.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\prarthana a r\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f865ddf5-4804-4e90-acd5-0cd3f06bdcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRARTHANA A R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a47c60e-81bd-4dfc-94ed-da75dc4ca33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f9b4c-4bd9-4b0f-82c0-40c6940c0fea",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "**Task:** Generate text using the prompt  \n",
    "\"The future of Artificial Intelligence is\"\n",
    "\n",
    "**Hypothesis:**  \n",
    "Encoder-only models (BERT, RoBERTa) will fail because they are not designed to generate new tokens.\n",
    "BART should succeed because it has a decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b6717c-d44f-4171-9c5f-19901fe5bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRARTHANA A R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PRARTHANA A R\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██| 202/202 [00:00<00:00, 237.70it/s, Materializing param=cls.predictions.transform.dense.weight]\n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is., \" ( \" \" ( \" \". ( ( )len \" \" \"................................................... a : the and, \" ( \", : the and and and - ( \" \" ( \". and in their so jefferson sea about that it it it in and and or some some some these the as earlier his \" ( ) ( \" ( \" ( ( \" ( \" \". it it it was ass ass square,......... : more actually so \". it was belle and \" \" \" ( \" it it something like let to work. it that were do who how the the the around c their, and my my my my ways we'- - -, - - - ( \". \" it to he many to her her \". it it and. me are of through through however, - - - [ to you ari about the on on us who his \" on it and and then family as education, in all the orne cabinet them back, and and and -, at this as my so business they, - - - ( ( ( :. him bar. \" of, \"... it, \". it (, in general thing in many so so\n",
      "\n",
      "Model: RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRARTHANA A R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PRARTHANA A R\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|█| 202/202 [00:01<00:00, 185.33it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is\n",
      "\n",
      "Model: BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRARTHANA A R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PRARTHANA A R\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|█| 159/159 [00:00<00:00, 228.20it/s, Materializing param=model.decoder.layers.5.self_attn_layer_n\n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence isコココ hots Gal diamondGamevertisGameGameansweredGameGame INFOGameansweredanswered customsGameGame Episode diamond diamond restrictsadish diamond rug Gal Representative cdGame standingGame standing vetted slots restrictsanswered restricts Falcons Falcons bin bin slots hoppingGameGame Xiao Representative Tues PlanetGamechnUnityGameGame nonsense saga Planet bin binanswered bin sequest animate bin bin bin Podcast Triple Tripletopiaanswered defines Hume slots Switch saga slots slots animate ultraviolet slots slotsUnityanswered Tues352BILITYoodle Your slots philosopherink sagainkeringductink shrimp INFOGameGame got animate animate animateoppable got pestic animate got bin animate saga meetings animate www bin saga hopping animateoppable hysteria ultraviolet ultravioletinkCertain animateoppable bin animateduct ultravioletPLAY slots TuesBILITY animate animate Your pestic slots meetings Enix Triple animate animateductBILITYduct reliable bin animate animate bin philosopher animateduct Enix Enix saga slotsoppable animateductduct bin Enix ultravioletink impressions saga animateduct animateoppable pestic bin Registry 27inkink animate animateAngel reliable pestic animate binLegal animate binAngel bin animateBILITYoppable binduct bin animate Registryduct FDR FDR FDR animateAngelowner pond animateAngeloppable sortingductVIEWductoppable binrenchesduct Enix www animateoppable animate coat coat binduct animateductoppableinkoppable animateoppable theorduct animate animateernandernandductoppableductduct admincastduct animateAngelductduct\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model)\n",
    "        result = generator(prompt, max_length=30)\n",
    "        print(result[0][\"generated_text\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98478f-6eb1-40b0-9b13-f8b455799fda",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment 2: Masked Language Modeling\n",
    "\n",
    "**Task:** Predict the missing word in:  \n",
    "\"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "**Hypothesis:**  \n",
    "BERT and RoBERTa should perform well because they were trained using Masked Language Modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8be1f11-3574-4b05-b6c6-f4bdfcba349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██| 202/202 [00:00<00:00, 211.76it/s, Materializing param=cls.predictions.transform.dense.weight]\n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5396891832351685,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575648844242096,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405455082654953,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.04451510310173035,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.017577266320586205,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "bert_fill(\"The goal of Generative AI is to [MASK] new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f905bafe-6d54-4488-bfb3-29d732b12709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 202/202 [00:00<00:00, 249.95it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.37112924456596375,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.3677125871181488,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.08351422846317291,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.02133500576019287,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.016521545127034187,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_fill = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "roberta_fill(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8baa5190-e002-42bd-9f70-1518e5079496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████| 259/259 [00:01<00:00, 246.74it/s, Materializing param=model.shared.weight]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0746145024895668,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571802496910095,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.06087947636842728,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.035935234278440475,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319424390792847,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_fill = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
    "bart_fill(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2147711-5265-4998-a10a-2c590d58a2c2",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering\n",
    "\n",
    "**Question:** What are the risks?  \n",
    "**Context:**  \n",
    "\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "**Note:**  \n",
    "The models used are base models and are not fine-tuned for question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76edd0b3-f576-4a6a-adc9-bd4b103cbfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 197/197 [00:00<00:00, 241.52it/s, Materializing param=bert.encoder.layer.11.output.dense.weigh\n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: , and deepfakes\n",
      "\n",
      "Model: RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 197/197 [00:01<00:00, 193.76it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: deepfakes.\n",
      "\n",
      "Model: BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████| 259/259 [00:01<00:00, 207.55it/s, Materializing param=model.shared.weight]\n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.bias   | MISSING | \n",
      "qa_outputs.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: bias, and deepfakes\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model)\n",
    "        answer = qa(question=question, context=context)\n",
    "        print(\"Answer:\", answer[\"answer\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03729db-7559-4a2c-9474-f85201bec42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
